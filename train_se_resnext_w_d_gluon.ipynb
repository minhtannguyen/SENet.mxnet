{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse,logging, os, math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "from common.util import download_file\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "from gluon_se_resnext_w_d_maxmin import se_resnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.gpus = '0,1,2,3' #the gpus will be used, e.g \"0,1,2,3\"\n",
    "        self.data_dir = '/tanData/datasets/imagenet/data/imagenet_senet' #the input data directory\n",
    "        self.log_dir = '/tanData/logs'\n",
    "        self.model_dir ='/tanData/models'\n",
    "        self.exp_name = 'exp1'\n",
    "        self.data_type = 'imagenet' #the dataset type\n",
    "        self.depth = 50 #the depth of resnet\n",
    "        self.batch_size = 192 #the batch size\n",
    "        self.num_group = 64 #the number of convolution groups\n",
    "        self.drop_out = 0.0 #the probability of an element to be zeroed\n",
    "        self.alpha_max = 0.5\n",
    "        self.alpha_min = 0.5\n",
    "        \n",
    "        self.list_dir = './' #the directory which contain the training list file\n",
    "        self.lr = 0.1 #initialization learning rate\n",
    "        self.mom = 0.9 #momentum for sgd\n",
    "        self.bn_mom = 0.9 #momentum for batch normlization\n",
    "        self.wd = 0.0001 #weight decay for sgd\n",
    "        self.workspace = 512 #memory space size(MB) used in convolution, \n",
    "                            #if xpu memory is oom, then you can try smaller vale, such as --workspace 256 \n",
    "        self.num_classes = 1000 #the class number of your task\n",
    "        self.aug_level = 2 # level 1: use only random crop and random mirror, \n",
    "                           #level 2: add scale/aspect/hsv augmentation based on level 1, \n",
    "                           #level 3: add rotation/shear augmentation based on level 2 \n",
    "        self.num_examples = 1281167 # the number of training examples\n",
    "        self.kv_store = 'device' # the kvstore type'\n",
    "        self.model_load_epoch = 0 # load the model on an epoch using the model-load-prefix\n",
    "        self.frequent = 50 # frequency of logging\n",
    "        self.memonger = False # true means using memonger to save momory, https://github.com/dmlc/mxnet-memonger\n",
    "        self.retrain = False # true means continue training\n",
    "        \n",
    "args = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-02 06:54:14,787 - <__main__.Options object at 0x7f5734605748>\n"
     ]
    }
   ],
   "source": [
    "hdlr = logging.FileHandler('./log/log-se-resnext-{}-{}.log'.format(args.data_type, args.depth))\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = mx.kvstore.create(args.kv_store)\n",
    "ctx = mx.cpu() if args.gpus is None else [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "batch_size = args.batch_size\n",
    "batch_size *= max(1, len(ctx))\n",
    "begin_epoch = args.model_load_epoch if args.model_load_epoch else 0\n",
    "if not os.path.exists(\"./model\"):\n",
    "    os.mkdir(\"./model\")\n",
    "model_prefix = \"seresnext_{}_{}_{}_{}\".format(args.data_type, args.depth, kv.rank, args.exp_name))\n",
    "# model_prefix = \"model/se-resnext-{}-{}-{}\".format(args.data_type, args.depth, kv.rank)\n",
    "arg_params = None\n",
    "aux_params = None\n",
    "if args.retrain:\n",
    "    _, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, args.model_load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"train.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"train_256_q90.rec\") if args.aug_level == 1\n",
    "                          else os.path.join(args.data_dir, \"train_480_q90.rec\") ,\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 224, 224),\n",
    "    batch_size          = batch_size,\n",
    "    pad                 = 4 if args.data_type == \"cifar10\" else 0,\n",
    "    fill_value          = 127,  # only used when pad is valid\n",
    "    rand_crop           = True,\n",
    "    max_random_scale    = 1.0,  # 480 with imagnet, 32 with cifar10\n",
    "    min_random_scale    = 1.0 if args.data_type == \"cifar10\" else 1.0 if args.aug_level == 1 else 0.533,  # 256.0/480.0=0.533, 256.0/384.0=0.667 256.0/256=1.0\n",
    "    max_aspect_ratio    = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 0.25, # 0.25\n",
    "    random_h            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 36,  # 0.4*90\n",
    "    random_s            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    random_l            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    max_rotate_angle    = 0 if args.aug_level <= 2 else 10,\n",
    "    max_shear_ratio     = 0 if args.aug_level <= 2 else 0.0, #0.1 args.aug_level = 3\n",
    "    rand_mirror         = True,\n",
    "    shuffle             = True,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank)\n",
    "val_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"val.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"val_256_q90.rec\"),\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    batch_size          = batch_size,\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 224, 224),\n",
    "    rand_crop           = False,\n",
    "    rand_mirror         = False,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_factor_scheduler(begin_epoch, epoch_size, step=[30, 60, 90, 95, 110, 120], factor=0.1):\n",
    "    step_ = [epoch_size * (x-begin_epoch) for x in step if x-begin_epoch > 0]\n",
    "    return mx.lr_scheduler.MultiFactorScheduler(step=step_, factor=factor) if len(step_) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(args.log_dir, args.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, val_data, ctx):\n",
    "    acc_top1_val = mx.metric.Accuracy()\n",
    "    acc_top5_val = mx.metric.TopKAccuracy(5)\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        for x in zip(data):\n",
    "            zmax, zmin = net(x)\n",
    "            z = args.alpha_max * zmax + args.alpha_min * zmin\n",
    "            outputs.append(z)\n",
    "            \n",
    "        acc_top1_val.update(label, outputs)\n",
    "        acc_top5_val.update(label, outputs)\n",
    "\n",
    "    _, top1 = acc_top1_val.get()\n",
    "    _, top5 = acc_top5_val.get()\n",
    "    return (top1, top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, val_data, num_epochs, ctx):\n",
    "    epoch_size = max(int(args.num_examples / args.batch_size / kv.num_workers), 1)\n",
    "    lr_sch = multi_factor_scheduler(begin_epoch, epoch_size, step=[30, 60, 90, 95, 110, 120], factor=0.1)\n",
    "    optimizer          ='nag',\n",
    "    optimizer_params   ={'learning_rate':args.lr, 'momentum':args.mom, 'wd':args.wd, 'lr_scheduler': lr_sch}\n",
    "    trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_top1_val = 0.\n",
    "    best_top5_val = 0.\n",
    "    log_interval = 1000\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_data.reset()\n",
    "        \n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        acc_top1.reset()\n",
    "        acc_top5.reset()\n",
    "        train_loss = 0\n",
    "        num_batch = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_data):\n",
    "            bs = batch.data[0].shape[0]\n",
    "            \n",
    "            data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "            label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            \n",
    "            loss = []\n",
    "            outputs = []\n",
    "            \n",
    "            with autograd.record():\n",
    "                for x, y in zip(data, label):\n",
    "                    zmax, zmin = net(x)\n",
    "                    loss_xent = args.alpha_max * criterion(zmax, label) + args.alpha_min * criterion(zmin, label)\n",
    "                    z = args.alpha_max * zmax + args.alpha_min * zmin\n",
    "\n",
    "                    loss.append(loss_xent )\n",
    "                    outputs.append(z)\n",
    "                    \n",
    "            for l in loss:\n",
    "                l.backward()\n",
    "                \n",
    "            trainer.step(bs)\n",
    "            \n",
    "            acc_top1.update(label, outputs)\n",
    "            acc_top5.update(label, outputs)\n",
    "            train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            num_batch += 1\n",
    "            if log_interval and not i % log_interval:\n",
    "                _, top1 = acc_top1.get()\n",
    "                _, top5 = acc_top5.get()\n",
    "                print('Epoch[%d] Batch [%d]     Speed: %f samples/sec   top1-acc=%f     top5-acc=%f'%(\n",
    "                          epoch, i, batch_size*log_interval/(time.time()-btic), top1, top5))\n",
    "                btic = time.time()\n",
    "        \n",
    "        _, top1 = acc_top1.get()\n",
    "        _, top5 = acc_top5.get()\n",
    "        train_loss /= num_batch * batch_size\n",
    "        writer.add_scalars('acc', {'train_top1': top1}, epoch)\n",
    "        writer.add_scalars('acc', {'train_top5': top5}, epoch)\n",
    "        \n",
    "        top1_val, top5_val = test(ctx, val_data)\n",
    "        \n",
    "        if top1_val > best_top1_val:\n",
    "            best_top1_val = top1_val\n",
    "            net.collect_params().save('%s/%s_best_top1.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_val > best_top5_val:\n",
    "            best_top5_val = top5_val\n",
    "            net.collect_params().save('%s/%s_best_top5.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        print('[Epoch %d] training: acc-top1=%f acc-top5=%f loss=%f'%(epoch, top1, top5, train_loss))\n",
    "        print('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        print('[Epoch %d] validation: acc-top1=%f acc-top5=%f'%(epoch, top1_val, top5_val))\n",
    "        \n",
    "        writer.add_scalars('acc', {'valid_top1': top1_val}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5': top5_val}, epoch)\n",
    "        \n",
    "        net.collect_params().save('%s/%s_epoch_%i.params'%(args.model_dir, args.exp_name, epoch))\n",
    "    \n",
    "    return best_top1_val, best_top5_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [0.25, 0.125, 0.0625, 0.03125]   # 1/4, 1/8, 1/16, 1/32\n",
    "    if args.depth == 18:\n",
    "        units = [2, 2, 2, 2]\n",
    "    elif args.depth == 34:\n",
    "        units = [3, 4, 6, 3]\n",
    "    elif args.depth == 50:\n",
    "        units = [3, 4, 6, 3]\n",
    "    elif args.depth == 101:\n",
    "        units = [3, 4, 23, 3]\n",
    "    elif args.depth == 152:\n",
    "        units = [3, 8, 36, 3]\n",
    "    elif args.depth == 200:\n",
    "        units = [3, 24, 36, 3]\n",
    "    elif args.depth == 269:\n",
    "        units = [3, 30, 48, 8]\n",
    "    else:\n",
    "        raise ValueError(\"no experiments done on detph {}, you can do it youself\".format(args.depth))\n",
    "\n",
    "    else:\n",
    "         raise ValueError(\"do not support {} yet\".format(args.data_type))\n",
    "\n",
    "num_epochs = 200 if args.data_type == \"cifar10\" else 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(ctx):        \n",
    "    model = se_resnext(units=units, num_stage=4, filter_list=[64, 256, 512, 1024, 2048] if args.depth >=50 else [64, 64, 128, 256, 512], ratio_list=ratio_list, num_class=args.num_classes, num_group=args.num_group, data_type=\"imagenet\", drop_out=args.drop_out, bn_mom=args.bn_mom)\n",
    "    \n",
    "    for param in model.collect_params().values():\n",
    "        if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "            if param.name.find('weight') != -1:\n",
    "                param.initialize(init=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        elif param.name.find('batchnorm') != -1 or param.name.find('instancenorm') != -1:\n",
    "            if param.name.find('gamma') != -1:\n",
    "                param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        elif param.name.find('biasadder') != -1:\n",
    "            param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        else:\n",
    "            param.initialize(init=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2), ctx=ctx)\n",
    "                \n",
    "        # model.hybridize()\n",
    "        \n",
    "    best_top1_val, best_top5_val = train(net=model, train_data=train_data, val_data=val_data, num_epochs=num_epochs, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(ctx=ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
