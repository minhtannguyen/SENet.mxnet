{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse,logging, os, math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "from gluon_se_resnext_w_d_maxmin import se_resnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.gpus = '0,1,2,3,4,5,6,7' #the gpus will be used, e.g \"0,1,2,3\"\n",
    "        self.data_dir = '/tanData/datasets/imagenet/data/imagenet_senet' #the input data directory\n",
    "        self.log_dir = '/tanData/logs'\n",
    "        self.model_dir ='/tanData/models'\n",
    "        self.exp_name = 'exp1'\n",
    "        self.data_type = 'imagenet' #the dataset type\n",
    "        self.depth = 50 #the depth of resnet\n",
    "        self.batch_size = 32 #the batch size\n",
    "        self.num_group = 64 #the number of convolution groups\n",
    "        self.drop_out = 0.0 #the probability of an element to be zeroed\n",
    "        self.alpha_max = 0.5\n",
    "        self.alpha_min = 0.5\n",
    "        \n",
    "        self.list_dir = './' #the directory which contain the training list file\n",
    "        self.lr = 0.1 #initialization learning rate\n",
    "        self.mom = 0.9 #momentum for sgd\n",
    "        self.bn_mom = 0.9 #momentum for batch normlization\n",
    "        self.wd = 0.0001 #weight decay for sgd\n",
    "        self.workspace = 512 #memory space size(MB) used in convolution, \n",
    "                            #if xpu memory is oom, then you can try smaller vale, such as --workspace 256 \n",
    "        self.num_classes = 1000 #the class number of your task\n",
    "        self.aug_level = 2 # level 1: use only random crop and random mirror, \n",
    "                           #level 2: add scale/aspect/hsv augmentation based on level 1, \n",
    "                           #level 3: add rotation/shear augmentation based on level 2 \n",
    "        self.num_examples = 1281167 # the number of training examples\n",
    "        self.kv_store = 'device' # the kvstore type'\n",
    "        self.model_load_epoch = 8 # load the model on an epoch using the model-load-prefix\n",
    "        self.frequent = 50 # frequency of logging\n",
    "        self.memonger = False # true means using memonger to save momory, https://github.com/dmlc/mxnet-memonger\n",
    "        self.retrain = False # true means continue training\n",
    "        \n",
    "args = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-04 03:55:29,673 - <__main__.Options object at 0x7fc08f3bb828>\n"
     ]
    }
   ],
   "source": [
    "hdlr = logging.FileHandler('./log/log-se-resnext-{}-{}.log'.format(args.data_type, args.depth))\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = mx.kvstore.create(args.kv_store)\n",
    "ctx = mx.cpu() if args.gpus is None else [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "batch_size = args.batch_size\n",
    "batch_size *= max(1, len(ctx))\n",
    "begin_epoch = args.model_load_epoch if args.model_load_epoch else 0\n",
    "if not os.path.exists(\"./model\"):\n",
    "    os.mkdir(\"./model\")\n",
    "model_prefix = \"seresnext_{}_{}_{}_{}\".format(args.data_type, args.depth, kv.rank, args.exp_name)\n",
    "# model_prefix = \"model/se-resnext-{}-{}-{}\".format(args.data_type, args.depth, kv.rank)\n",
    "arg_params = None\n",
    "aux_params = None\n",
    "if args.retrain:\n",
    "    _, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, args.model_load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"train.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"train_256_q90.rec\") if args.aug_level == 1\n",
    "                          else os.path.join(args.data_dir, \"train_480_q90.rec\") ,\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 224, 224),\n",
    "    batch_size          = batch_size,\n",
    "    pad                 = 4 if args.data_type == \"cifar10\" else 0,\n",
    "    fill_value          = 127,  # only used when pad is valid\n",
    "    rand_crop           = True,\n",
    "    max_random_scale    = 1.0,  # 480 with imagnet, 32 with cifar10\n",
    "    min_random_scale    = 1.0 if args.data_type == \"cifar10\" else 1.0 if args.aug_level == 1 else 0.533,  # 256.0/480.0=0.533, 256.0/384.0=0.667 256.0/256=1.0\n",
    "    max_aspect_ratio    = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 0.25, # 0.25\n",
    "    random_h            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 36,  # 0.4*90\n",
    "    random_s            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    random_l            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    max_rotate_angle    = 0 if args.aug_level <= 2 else 10,\n",
    "    max_shear_ratio     = 0 if args.aug_level <= 2 else 0.0, #0.1 args.aug_level = 3\n",
    "    rand_mirror         = True,\n",
    "    shuffle             = True,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank)\n",
    "val_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"val.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"val_256_q90.rec\"),\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    batch_size          = batch_size,\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 224, 224),\n",
    "    rand_crop           = False,\n",
    "    rand_mirror         = False,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_factor_scheduler(begin_epoch, epoch_size, step=[30, 60, 90, 95, 110, 120], factor=0.1):\n",
    "    step_ = [epoch_size * (x-begin_epoch) for x in step if x-begin_epoch > 0]\n",
    "    return mx.lr_scheduler.MultiFactorScheduler(step=step_, factor=factor) if len(step_) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(args.log_dir, args.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, val_data, ctx):\n",
    "    val_data.reset()\n",
    "    \n",
    "    acc_top1_val = mx.metric.Accuracy()\n",
    "    acc_top5_val = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val_max = mx.metric.Accuracy()\n",
    "    acc_top5_val_max = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val_min = mx.metric.Accuracy()\n",
    "    acc_top5_val_min = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val.reset()\n",
    "    acc_top5_val.reset()\n",
    "    acc_top1_val_max.reset()\n",
    "    acc_top5_val_max.reset()\n",
    "    acc_top1_val_min.reset()\n",
    "    acc_top5_val_min.reset()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        \n",
    "        outputs = []\n",
    "        outputsmax = []\n",
    "        outputsmin = []\n",
    "        for x in data:\n",
    "            zmax, zmin = net(x)\n",
    "            z = args.alpha_max * zmax + args.alpha_min * zmin\n",
    "            outputs.append(z)\n",
    "            outputsmax.append(zmax)\n",
    "            outputsmin.append(zmin)\n",
    "            \n",
    "            \n",
    "        acc_top1_val.update(label, outputs)\n",
    "        acc_top5_val.update(label, outputs)\n",
    "        acc_top1_val_max.update(label, outputsmax)\n",
    "        acc_top5_val_max.update(label, outputsmax)\n",
    "        acc_top1_val_min.update(label, outputsmin)\n",
    "        acc_top5_val_min.update(label, outputsmin)\n",
    "\n",
    "    _, top1 = acc_top1_val.get()\n",
    "    _, top5 = acc_top5_val.get()\n",
    "    _, top1max = acc_top1_val_max.get()\n",
    "    _, top5max = acc_top5_val_max.get()\n",
    "    _, top1min = acc_top1_val_min.get()\n",
    "    _, top5min = acc_top5_val_min.get()\n",
    "    return (top1, top5, top1max, top5max, top1min, top5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, val_data, num_epochs, ctx):\n",
    "    epoch_size = max(int(args.num_examples / batch_size / kv.num_workers), 1)\n",
    "    lr_sch = multi_factor_scheduler(begin_epoch, epoch_size, step=[30, 60, 90, 95, 110, 120], factor=0.1)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'nag', {'learning_rate':args.lr, 'momentum':args.mom, 'wd':args.wd, 'lr_scheduler': lr_sch})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_top1_val = 0.; best_top1_valmax = 0.; best_top1_valmin = 0.\n",
    "    best_top5_val = 0.; best_top5_valmax = 0.; best_top5_valmin = 0.\n",
    "    log_interval = 500\n",
    "    \n",
    "    for epoch in range(begin_epoch, num_epochs):\n",
    "        train_data.reset()\n",
    "        \n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        acc_top1.reset()\n",
    "        acc_top5.reset()\n",
    "        train_loss = 0\n",
    "        num_batch = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_data):\n",
    "            bs = batch.data[0].shape[0]\n",
    "            \n",
    "            data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "            label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            \n",
    "            loss = []\n",
    "            outputs = []\n",
    "            \n",
    "            with autograd.record():\n",
    "                for x, y in zip(data, label):\n",
    "                    zmax, zmin = net(x)\n",
    "                    loss_xent = args.alpha_max * criterion(zmax, y) + args.alpha_min * criterion(zmin, y)\n",
    "                    z = args.alpha_max * zmax + args.alpha_min * zmin\n",
    "\n",
    "                    loss.append(loss_xent)\n",
    "                    outputs.append(z)\n",
    "                    \n",
    "            for l in loss:\n",
    "                l.backward()\n",
    "                \n",
    "            trainer.step(bs)\n",
    "            \n",
    "            acc_top1.update(label, outputs)\n",
    "            acc_top5.update(label, outputs)\n",
    "            train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            num_batch += 1\n",
    "            if log_interval and not i % log_interval:\n",
    "                _, top1 = acc_top1.get()\n",
    "                _, top5 = acc_top5.get()\n",
    "                logging.info('Epoch[%d] Batch [%d] Lr: %f     Speed: %f samples/sec   top1-acc=%f     top5-acc=%f'%(\n",
    "                          epoch, i, trainer.learning_rate, batch_size*log_interval/(time.time()-btic), top1, top5))\n",
    "                btic = time.time()\n",
    "        \n",
    "        _, top1 = acc_top1.get()\n",
    "        _, top5 = acc_top5.get()\n",
    "        train_loss /= num_batch * batch_size\n",
    "        writer.add_scalars('acc', {'train_top1': top1}, epoch)\n",
    "        writer.add_scalars('acc', {'train_top5': top5}, epoch)\n",
    "        \n",
    "        top1_val, top5_val, top1_valmax, top5_valmax, top1_valmin, top5_valmin = test(net=net, val_data=val_data, ctx=ctx)\n",
    "        \n",
    "        if top1_val > best_top1_val:\n",
    "            best_top1_val = top1_val\n",
    "            net.collect_params().save('%s/%s_best_top1.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top1_valmax > best_top1_valmax:\n",
    "            best_top1_valmax = top1_valmax\n",
    "            net.collect_params().save('%s/%s_best_top1_max.params'%(args.model_dir, model_prefix))\n",
    "            \n",
    "        if top1_valmin > best_top1_valmin:\n",
    "            best_top1_valmin = top1_valmin\n",
    "            net.collect_params().save('%s/%s_best_top1_min.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_val > best_top5_val:\n",
    "            best_top5_val = top5_val\n",
    "            net.collect_params().save('%s/%s_best_top5.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_valmax > best_top5_valmax:\n",
    "            best_top5_valmax = top5_valmax\n",
    "            net.collect_params().save('%s/%s_best_top5_max.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_valmin > best_top5_valmin:\n",
    "            best_top5_valmin = top5_valmin\n",
    "            net.collect_params().save('%s/%s_best_top5_min.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        logging.info('[Epoch %d] training: acc-top1=%f acc-top5=%f loss=%f lr=%f'%(epoch, top1, top5, train_loss, trainer.learning_rate))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        logging.info('[Epoch %d] validation: acc-top1=%f acc-top5=%f best-acc-top1=%f best-acc-top5=%f'%(epoch, top1_val, top5_val, best_top1_val, best_top5_val))\n",
    "        logging.info('[Epoch %d] validation: acc-top1-max=%f acc-top5-max=%f best-acc-top1-max=%f best-acc-top5-max=%f'%(epoch, top1_valmax, top5_valmax, best_top1_valmax, best_top5_valmax))\n",
    "        logging.info('[Epoch %d] validation: acc-top1-min=%f acc-top5-min=%f best-acc-top1-min=%f best-acc-top5-min=%f'%(epoch, top1_valmin, top5_valmin, best_top1_valmin, best_top5_valmin))\n",
    "        \n",
    "        writer.add_scalars('acc', {'valid_top1': top1_val}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5': top5_val}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top1_max': top1_valmax}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5_max': top5_valmax}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top1_min': top1_valmin}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5_min': top5_valmin}, epoch)\n",
    "        \n",
    "        net.collect_params().save('%s/%s_current.params'%(args.model_dir, model_prefix))\n",
    "        if not epoch % 10:\n",
    "            net.collect_params().save('%s/%s_epoch_%i.params'%(args.model_dir, model_prefix, epoch))\n",
    "    \n",
    "    return best_top1_val, best_top5_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [0.25, 0.125, 0.0625, 0.03125]   # 1/4, 1/8, 1/16, 1/32\n",
    "if args.depth == 18:\n",
    "    units = [2, 2, 2, 2]\n",
    "elif args.depth == 34:\n",
    "    units = [3, 4, 6, 3]\n",
    "elif args.depth == 50:\n",
    "    units = [3, 4, 6, 3]\n",
    "elif args.depth == 101:\n",
    "    units = [3, 4, 23, 3]\n",
    "elif args.depth == 152:\n",
    "    units = [3, 8, 36, 3]\n",
    "elif args.depth == 200:\n",
    "    units = [3, 24, 36, 3]\n",
    "elif args.depth == 269:\n",
    "    units = [3, 30, 48, 8]\n",
    "else:\n",
    "    raise ValueError(\"no experiments done on detph {}, you can do it youself\".format(args.depth))\n",
    "\n",
    "num_epochs = 200 if args.data_type == \"cifar10\" else 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(ctx):        \n",
    "    model = se_resnext(units=units, num_stage=4, filter_list=[64, 256, 512, 1024, 2048] if args.depth >=50 else [64, 64, 128, 256, 512], ratio_list=ratio_list, num_class=args.num_classes, num_group=args.num_group, data_type=\"imagenet\", drop_out=args.drop_out, bn_mom=args.bn_mom)\n",
    "    model.collect_params().load('/tanData/models/seresnext_imagenet_50_0_exp1_current.params', ctx=ctx)\n",
    "#     for param in model.collect_params().values():\n",
    "#         if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "#             if param.name.find('weight') != -1:\n",
    "#                 param.initialize(init=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2), ctx=ctx)\n",
    "#             else:\n",
    "#                 param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "#         elif param.name.find('batchnorm') != -1:\n",
    "#             if param.name.find('gamma') != -1:\n",
    "#                 param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "#             else:\n",
    "#                 param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "#         elif param.name.find('biasadder') != -1:\n",
    "#             param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "#         else:\n",
    "#             param.initialize(init=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2), ctx=ctx)\n",
    "                  \n",
    "    model.hybridize()\n",
    "        \n",
    "    best_top1_val, best_top5_val = train(net=model, train_data=train_data, val_data=val_data, num_epochs=num_epochs, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-04 03:56:23,964 - Epoch[8] Batch [0] Lr: 0.100000     Speed: 3939.521365 samples/sec   top1-acc=0.457031     top5-acc=0.695312\n",
      "2018-09-04 04:03:53,888 - Epoch[8] Batch [500] Lr: 0.100000     Speed: 284.497977 samples/sec   top1-acc=0.455651     top5-acc=0.701456\n",
      "2018-09-04 04:11:19,034 - Epoch[8] Batch [1000] Lr: 0.100000     Speed: 287.554524 samples/sec   top1-acc=0.454421     top5-acc=0.701026\n",
      "2018-09-04 04:18:45,976 - Epoch[8] Batch [1500] Lr: 0.100000     Speed: 286.398349 samples/sec   top1-acc=0.455514     top5-acc=0.701938\n",
      "2018-09-04 04:26:09,098 - Epoch[8] Batch [2000] Lr: 0.100000     Speed: 288.863464 samples/sec   top1-acc=0.454821     top5-acc=0.701977\n",
      "2018-09-04 04:33:30,467 - Epoch[8] Batch [2500] Lr: 0.100000     Speed: 290.014732 samples/sec   top1-acc=0.454801     top5-acc=0.701641\n",
      "2018-09-04 04:40:55,000 - Epoch[8] Batch [3000] Lr: 0.100000     Speed: 287.947042 samples/sec   top1-acc=0.455490     top5-acc=0.701825\n",
      "2018-09-04 04:48:23,477 - Epoch[8] Batch [3500] Lr: 0.100000     Speed: 285.414255 samples/sec   top1-acc=0.455921     top5-acc=0.702278\n",
      "2018-09-04 04:55:43,683 - Epoch[8] Batch [4000] Lr: 0.100000     Speed: 290.778792 samples/sec   top1-acc=0.456258     top5-acc=0.702654\n",
      "2018-09-04 05:03:05,792 - Epoch[8] Batch [4500] Lr: 0.100000     Speed: 289.526490 samples/sec   top1-acc=0.456398     top5-acc=0.702747\n",
      "2018-09-04 05:10:26,710 - Epoch[8] Batch [5000] Lr: 0.100000     Speed: 290.309221 samples/sec   top1-acc=0.456930     top5-acc=0.703141\n",
      "2018-09-04 05:11:58,031 - [Epoch 8] training: acc-top1=0.456920 acc-top5=0.703130 loss=2.622483 lr=0.100000\n",
      "2018-09-04 05:11:58,032 - [Epoch 8] time cost: 4566.559796\n",
      "2018-09-04 05:11:58,033 - [Epoch 8] validation: acc-top1=0.499601 acc-top5=0.755142 best-acc-top1=0.499601 best-acc-top5=0.755142\n",
      "2018-09-04 05:11:58,034 - [Epoch 8] validation: acc-top1-max=0.459104 acc-top5-max=0.717514 best-acc-top1-max=0.459104 best-acc-top5-max=0.717514\n",
      "2018-09-04 05:11:58,035 - [Epoch 8] validation: acc-top1-min=0.473812 acc-top5-min=0.731585 best-acc-top1-min=0.473812 best-acc-top5-min=0.731585\n",
      "2018-09-04 05:11:59,887 - Epoch[9] Batch [0] Lr: 0.100000     Speed: 94084.716232 samples/sec   top1-acc=0.460938     top5-acc=0.691406\n",
      "2018-09-04 05:19:21,851 - Epoch[9] Batch [500] Lr: 0.100000     Speed: 289.621015 samples/sec   top1-acc=0.462138     top5-acc=0.708817\n",
      "2018-09-04 05:26:46,323 - Epoch[9] Batch [1000] Lr: 0.100000     Speed: 287.986031 samples/sec   top1-acc=0.461581     top5-acc=0.707519\n",
      "2018-09-04 05:34:11,231 - Epoch[9] Batch [1500] Lr: 0.100000     Speed: 287.704803 samples/sec   top1-acc=0.462574     top5-acc=0.707528\n",
      "2018-09-04 05:41:32,528 - Epoch[9] Batch [2000] Lr: 0.100000     Speed: 290.063662 samples/sec   top1-acc=0.462771     top5-acc=0.707679\n",
      "2018-09-04 05:48:51,920 - Epoch[9] Batch [2500] Lr: 0.100000     Speed: 291.317489 samples/sec   top1-acc=0.462448     top5-acc=0.707617\n",
      "2018-09-04 05:56:15,017 - Epoch[9] Batch [3000] Lr: 0.100000     Speed: 288.880554 samples/sec   top1-acc=0.462364     top5-acc=0.707583\n",
      "2018-09-04 06:03:39,918 - Epoch[9] Batch [3500] Lr: 0.100000     Speed: 287.708857 samples/sec   top1-acc=0.462758     top5-acc=0.708206\n",
      "2018-09-04 06:11:02,303 - Epoch[9] Batch [4000] Lr: 0.100000     Speed: 289.345640 samples/sec   top1-acc=0.462983     top5-acc=0.708542\n",
      "2018-09-04 06:18:26,019 - Epoch[9] Batch [4500] Lr: 0.100000     Speed: 288.478126 samples/sec   top1-acc=0.463328     top5-acc=0.708865\n",
      "2018-09-04 06:25:52,726 - Epoch[9] Batch [5000] Lr: 0.100000     Speed: 286.544610 samples/sec   top1-acc=0.463596     top5-acc=0.709127\n",
      "2018-09-04 06:27:14,529 - [Epoch 9] training: acc-top1=0.463597 acc-top5=0.709128 loss=2.586322 lr=0.100000\n",
      "2018-09-04 06:27:14,531 - [Epoch 9] time cost: 4516.005097\n",
      "2018-09-04 06:27:14,532 - [Epoch 9] validation: acc-top1=0.513502 acc-top5=0.766627 best-acc-top1=0.513502 best-acc-top5=0.766627\n",
      "2018-09-04 06:27:14,533 - [Epoch 9] validation: acc-top1-max=0.473818 acc-top5-max=0.730849 best-acc-top1-max=0.473818 best-acc-top5-max=0.730849\n",
      "2018-09-04 06:27:14,534 - [Epoch 9] validation: acc-top1-min=0.472897 acc-top5-min=0.734936 best-acc-top1-min=0.473812 best-acc-top5-min=0.734936\n",
      "2018-09-04 06:27:16,465 - Epoch[10] Batch [0] Lr: 0.100000     Speed: 98390.968873 samples/sec   top1-acc=0.500000     top5-acc=0.726562\n",
      "2018-09-04 06:34:32,885 - Epoch[10] Batch [500] Lr: 0.100000     Speed: 293.300693 samples/sec   top1-acc=0.468235     top5-acc=0.713893\n",
      "2018-09-04 06:41:56,484 - Epoch[10] Batch [1000] Lr: 0.100000     Speed: 288.551216 samples/sec   top1-acc=0.468165     top5-acc=0.713455\n",
      "2018-09-04 06:49:13,908 - Epoch[10] Batch [1500] Lr: 0.100000     Speed: 292.627493 samples/sec   top1-acc=0.469044     top5-acc=0.713584\n",
      "2018-09-04 06:56:36,462 - Epoch[10] Batch [2000] Lr: 0.100000     Speed: 289.235021 samples/sec   top1-acc=0.468430     top5-acc=0.713594\n",
      "2018-09-04 07:04:00,851 - Epoch[10] Batch [2500] Lr: 0.100000     Speed: 288.038361 samples/sec   top1-acc=0.468839     top5-acc=0.713838\n",
      "2018-09-04 07:11:22,491 - Epoch[10] Batch [3000] Lr: 0.100000     Speed: 289.833205 samples/sec   top1-acc=0.468612     top5-acc=0.713774\n",
      "2018-09-04 07:18:44,568 - Epoch[10] Batch [3500] Lr: 0.100000     Speed: 289.545983 samples/sec   top1-acc=0.469687     top5-acc=0.714488\n",
      "2018-09-04 07:26:05,763 - Epoch[10] Batch [4000] Lr: 0.100000     Speed: 290.124372 samples/sec   top1-acc=0.469937     top5-acc=0.714663\n",
      "2018-09-04 07:33:32,245 - Epoch[10] Batch [4500] Lr: 0.100000     Speed: 286.689698 samples/sec   top1-acc=0.470075     top5-acc=0.714852\n",
      "2018-09-04 07:40:55,591 - Epoch[10] Batch [5000] Lr: 0.100000     Speed: 288.717608 samples/sec   top1-acc=0.470293     top5-acc=0.714977\n",
      "2018-09-04 07:42:16,672 - [Epoch 10] training: acc-top1=0.470278 acc-top5=0.714957 loss=2.551497 lr=0.100000\n",
      "2018-09-04 07:42:16,674 - [Epoch 10] time cost: 4501.510300\n",
      "2018-09-04 07:42:16,675 - [Epoch 10] validation: acc-top1=0.518630 acc-top5=0.771014 best-acc-top1=0.518630 best-acc-top5=0.771014\n",
      "2018-09-04 07:42:16,676 - [Epoch 10] validation: acc-top1-max=0.481911 acc-top5-max=0.737260 best-acc-top1-max=0.481911 best-acc-top5-max=0.737260\n",
      "2018-09-04 07:42:16,676 - [Epoch 10] validation: acc-top1-min=0.494471 acc-top5-min=0.750240 best-acc-top1-min=0.494471 best-acc-top5-min=0.750240\n",
      "2018-09-04 07:42:19,072 - Epoch[11] Batch [0] Lr: 0.100000     Speed: 82680.915228 samples/sec   top1-acc=0.460938     top5-acc=0.691406\n",
      "2018-09-04 07:49:40,567 - Epoch[11] Batch [500] Lr: 0.100000     Speed: 289.928577 samples/sec   top1-acc=0.472960     top5-acc=0.719421\n",
      "2018-09-04 07:57:07,569 - Epoch[11] Batch [1000] Lr: 0.100000     Speed: 286.355915 samples/sec   top1-acc=0.472461     top5-acc=0.718734\n",
      "2018-09-04 08:04:35,436 - Epoch[11] Batch [1500] Lr: 0.100000     Speed: 285.802425 samples/sec   top1-acc=0.473674     top5-acc=0.718974\n",
      "2018-09-04 08:11:57,835 - Epoch[11] Batch [2000] Lr: 0.100000     Speed: 289.336163 samples/sec   top1-acc=0.473708     top5-acc=0.718963\n",
      "2018-09-04 08:19:24,037 - Epoch[11] Batch [2500] Lr: 0.100000     Speed: 286.871066 samples/sec   top1-acc=0.473706     top5-acc=0.718739\n",
      "2018-09-04 08:26:45,567 - Epoch[11] Batch [3000] Lr: 0.100000     Speed: 289.907205 samples/sec   top1-acc=0.473936     top5-acc=0.718551\n",
      "2018-09-04 08:34:11,620 - Epoch[11] Batch [3500] Lr: 0.100000     Speed: 286.966817 samples/sec   top1-acc=0.474463     top5-acc=0.718785\n",
      "2018-09-04 08:41:34,726 - Epoch[11] Batch [4000] Lr: 0.100000     Speed: 288.873414 samples/sec   top1-acc=0.474871     top5-acc=0.718916\n",
      "2018-09-04 08:49:01,020 - Epoch[11] Batch [4500] Lr: 0.100000     Speed: 286.811292 samples/sec   top1-acc=0.475144     top5-acc=0.719122\n",
      "2018-09-04 08:56:21,462 - Epoch[11] Batch [5000] Lr: 0.100000     Speed: 290.622653 samples/sec   top1-acc=0.475422     top5-acc=0.719289\n",
      "2018-09-04 08:57:41,607 - [Epoch 11] training: acc-top1=0.475412 acc-top5=0.719271 loss=2.526174 lr=0.100000\n",
      "2018-09-04 08:57:41,609 - [Epoch 11] time cost: 4524.085410\n",
      "2018-09-04 08:57:41,610 - [Epoch 11] validation: acc-top1=0.522660 acc-top5=0.772541 best-acc-top1=0.522660 best-acc-top5=0.772541\n",
      "2018-09-04 08:57:41,611 - [Epoch 11] validation: acc-top1-max=0.466159 acc-top5-max=0.724649 best-acc-top1-max=0.481911 best-acc-top5-max=0.737260\n",
      "2018-09-04 08:57:41,612 - [Epoch 11] validation: acc-top1-min=0.491271 acc-top5-min=0.749562 best-acc-top1-min=0.494471 best-acc-top5-min=0.750240\n",
      "2018-09-04 08:57:43,770 - Epoch[12] Batch [0] Lr: 0.100000     Speed: 83624.192548 samples/sec   top1-acc=0.503906     top5-acc=0.699219\n",
      "2018-09-04 09:05:02,464 - Epoch[12] Batch [500] Lr: 0.100000     Speed: 291.780523 samples/sec   top1-acc=0.479658     top5-acc=0.722882\n",
      "2018-09-04 09:12:23,306 - Epoch[12] Batch [1000] Lr: 0.100000     Speed: 290.359647 samples/sec   top1-acc=0.478291     top5-acc=0.722750\n",
      "2018-09-04 09:19:46,829 - Epoch[12] Batch [1500] Lr: 0.100000     Speed: 288.601066 samples/sec   top1-acc=0.479480     top5-acc=0.723663\n",
      "2018-09-04 09:27:11,995 - Epoch[12] Batch [2000] Lr: 0.100000     Speed: 287.538438 samples/sec   top1-acc=0.478850     top5-acc=0.722801\n",
      "2018-09-04 09:34:34,051 - Epoch[12] Batch [2500] Lr: 0.100000     Speed: 289.562552 samples/sec   top1-acc=0.478379     top5-acc=0.722367\n",
      "2018-09-04 09:42:01,482 - Epoch[12] Batch [3000] Lr: 0.100000     Speed: 286.082173 samples/sec   top1-acc=0.478303     top5-acc=0.722405\n",
      "2018-09-04 09:49:30,313 - Epoch[12] Batch [3500] Lr: 0.100000     Speed: 285.191871 samples/sec   top1-acc=0.478954     top5-acc=0.722817\n",
      "2018-09-04 09:56:59,859 - Epoch[12] Batch [4000] Lr: 0.100000     Speed: 284.737996 samples/sec   top1-acc=0.479142     top5-acc=0.722946\n",
      "2018-09-04 10:04:25,258 - Epoch[12] Batch [4500] Lr: 0.100000     Speed: 287.387968 samples/sec   top1-acc=0.479171     top5-acc=0.722959\n",
      "2018-09-04 10:11:51,602 - Epoch[12] Batch [5000] Lr: 0.100000     Speed: 286.780514 samples/sec   top1-acc=0.479437     top5-acc=0.723228\n",
      "2018-09-04 10:13:12,815 - [Epoch 12] training: acc-top1=0.479436 acc-top5=0.723215 loss=2.504156 lr=0.100000\n",
      "2018-09-04 10:13:12,818 - [Epoch 12] time cost: 4530.578012\n",
      "2018-09-04 10:13:12,819 - [Epoch 12] validation: acc-top1=0.527524 acc-top5=0.778766 best-acc-top1=0.527524 best-acc-top5=0.778766\n",
      "2018-09-04 10:13:12,820 - [Epoch 12] validation: acc-top1-max=0.479748 acc-top5-max=0.739323 best-acc-top1-max=0.481911 best-acc-top5-max=0.739323\n",
      "2018-09-04 10:13:12,821 - [Epoch 12] validation: acc-top1-min=0.499439 acc-top5-min=0.755349 best-acc-top1-min=0.499439 best-acc-top5-min=0.755349\n",
      "2018-09-04 10:13:14,742 - Epoch[13] Batch [0] Lr: 0.100000     Speed: 99085.501819 samples/sec   top1-acc=0.437500     top5-acc=0.710938\n",
      "2018-09-04 10:20:39,512 - Epoch[13] Batch [500] Lr: 0.100000     Speed: 287.795974 samples/sec   top1-acc=0.482940     top5-acc=0.726360\n",
      "2018-09-04 10:28:02,000 - Epoch[13] Batch [1000] Lr: 0.100000     Speed: 289.275699 samples/sec   top1-acc=0.481897     top5-acc=0.725727\n",
      "2018-09-04 10:35:24,837 - Epoch[13] Batch [1500] Lr: 0.100000     Speed: 289.050941 samples/sec   top1-acc=0.482379     top5-acc=0.725790\n",
      "2018-09-04 10:42:50,582 - Epoch[13] Batch [2000] Lr: 0.100000     Speed: 287.165314 samples/sec   top1-acc=0.481929     top5-acc=0.725825\n",
      "2018-09-04 10:50:10,576 - Epoch[13] Batch [2500] Lr: 0.100000     Speed: 290.918222 samples/sec   top1-acc=0.482124     top5-acc=0.725635\n",
      "2018-09-04 10:57:32,700 - Epoch[13] Batch [3000] Lr: 0.100000     Speed: 289.515363 samples/sec   top1-acc=0.481934     top5-acc=0.725496\n",
      "2018-09-04 11:04:54,968 - Epoch[13] Batch [3500] Lr: 0.100000     Speed: 289.421437 samples/sec   top1-acc=0.482217     top5-acc=0.725683\n",
      "2018-09-04 11:12:15,033 - Epoch[13] Batch [4000] Lr: 0.100000     Speed: 290.871143 samples/sec   top1-acc=0.482376     top5-acc=0.725825\n",
      "2018-09-04 11:19:38,839 - Epoch[13] Batch [4500] Lr: 0.100000     Speed: 288.419560 samples/sec   top1-acc=0.482504     top5-acc=0.725959\n",
      "2018-09-04 11:26:57,706 - Epoch[13] Batch [5000] Lr: 0.100000     Speed: 291.661936 samples/sec   top1-acc=0.482743     top5-acc=0.726102\n",
      "2018-09-04 11:28:18,296 - [Epoch 13] training: acc-top1=0.482749 acc-top5=0.726086 loss=2.486733 lr=0.100000\n",
      "2018-09-04 11:28:18,299 - [Epoch 13] time cost: 4504.849142\n",
      "2018-09-04 11:28:18,300 - [Epoch 13] validation: acc-top1=0.537300 acc-top5=0.788822 best-acc-top1=0.537300 best-acc-top5=0.788822\n",
      "2018-09-04 11:28:18,301 - [Epoch 13] validation: acc-top1-max=0.491106 acc-top5-max=0.753486 best-acc-top1-max=0.491106 best-acc-top5-max=0.753486\n",
      "2018-09-04 11:28:18,303 - [Epoch 13] validation: acc-top1-min=0.518349 acc-top5-min=0.771915 best-acc-top1-min=0.518349 best-acc-top5-min=0.771915\n",
      "2018-09-04 11:28:20,230 - Epoch[14] Batch [0] Lr: 0.100000     Speed: 98405.342367 samples/sec   top1-acc=0.519531     top5-acc=0.718750\n",
      "2018-09-04 11:35:42,718 - Epoch[14] Batch [500] Lr: 0.100000     Speed: 289.279746 samples/sec   top1-acc=0.485451     top5-acc=0.731093\n",
      "2018-09-04 11:43:08,581 - Epoch[14] Batch [1000] Lr: 0.100000     Speed: 287.088801 samples/sec   top1-acc=0.484687     top5-acc=0.728943\n",
      "2018-09-04 11:50:32,186 - Epoch[14] Batch [1500] Lr: 0.100000     Speed: 288.550315 samples/sec   top1-acc=0.485307     top5-acc=0.729126\n",
      "2018-09-04 11:57:52,176 - Epoch[14] Batch [2000] Lr: 0.100000     Speed: 290.919870 samples/sec   top1-acc=0.484865     top5-acc=0.728821\n",
      "2018-09-04 12:05:13,491 - Epoch[14] Batch [2500] Lr: 0.100000     Speed: 290.048073 samples/sec   top1-acc=0.484780     top5-acc=0.728534\n",
      "2018-09-04 12:12:32,551 - Epoch[14] Batch [3000] Lr: 0.100000     Speed: 291.538018 samples/sec   top1-acc=0.485044     top5-acc=0.728679\n",
      "2018-09-04 12:19:54,276 - Epoch[14] Batch [3500] Lr: 0.100000     Speed: 289.777134 samples/sec   top1-acc=0.485415     top5-acc=0.728917\n",
      "2018-09-04 12:27:21,086 - Epoch[14] Batch [4000] Lr: 0.100000     Speed: 286.480323 samples/sec   top1-acc=0.485646     top5-acc=0.729151\n",
      "2018-09-04 12:34:44,508 - Epoch[14] Batch [4500] Lr: 0.100000     Speed: 288.670111 samples/sec   top1-acc=0.485781     top5-acc=0.729238\n",
      "2018-09-04 12:42:05,864 - Epoch[14] Batch [5000] Lr: 0.100000     Speed: 290.021398 samples/sec   top1-acc=0.485935     top5-acc=0.729452\n",
      "2018-09-04 12:43:30,283 - [Epoch 14] training: acc-top1=0.485928 acc-top5=0.729445 loss=2.469040 lr=0.100000\n",
      "2018-09-04 12:43:30,285 - [Epoch 14] time cost: 4511.355877\n",
      "2018-09-04 12:43:30,286 - [Epoch 14] validation: acc-top1=0.529177 acc-top5=0.781250 best-acc-top1=0.537300 best-acc-top5=0.788822\n",
      "2018-09-04 12:43:30,286 - [Epoch 14] validation: acc-top1-max=0.499522 acc-top5-max=0.757693 best-acc-top1-max=0.499522 best-acc-top5-max=0.757693\n",
      "2018-09-04 12:43:30,287 - [Epoch 14] validation: acc-top1-min=0.482980 acc-top5-min=0.743563 best-acc-top1-min=0.518349 best-acc-top5-min=0.771915\n",
      "2018-09-04 12:43:32,123 - Epoch[15] Batch [0] Lr: 0.100000     Speed: 98678.749173 samples/sec   top1-acc=0.441406     top5-acc=0.710938\n",
      "2018-09-04 12:50:54,256 - Epoch[15] Batch [500] Lr: 0.100000     Speed: 289.510513 samples/sec   top1-acc=0.489716     top5-acc=0.730905\n",
      "2018-09-04 12:58:15,579 - Epoch[15] Batch [1000] Lr: 0.100000     Speed: 290.041885 samples/sec   top1-acc=0.488145     top5-acc=0.730672\n",
      "2018-09-04 13:05:38,463 - Epoch[15] Batch [1500] Lr: 0.100000     Speed: 289.017802 samples/sec   top1-acc=0.488947     top5-acc=0.731166\n",
      "2018-09-04 13:13:01,120 - Epoch[15] Batch [2000] Lr: 0.100000     Speed: 289.166647 samples/sec   top1-acc=0.488527     top5-acc=0.731056\n",
      "2018-09-04 13:20:22,054 - Epoch[15] Batch [2500] Lr: 0.100000     Speed: 290.296843 samples/sec   top1-acc=0.488583     top5-acc=0.731019\n"
     ]
    }
   ],
   "source": [
    "run_train(ctx=ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
